# -*- coding: utf-8 -*-
"""water quality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dnTvimlo-1NmafrJfuxq8pxwdxn1yDkK
"""

import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from tqdm import tqdm_notebook
import plotly.figure_factory as ff

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os
for dirname, _, filenames in os.walk('/content/water_potability.csv'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

df = pd.read_csv("/content/water_potability.csv")
df.head()

df.describe()

df.info()

print("there are total {} data points and {} features in the dataframe". format(df.shape[0],df.shape[1]))

#null values

sns.heatmap(df.isnull())

for i in df.columns:
  if df[i].isnull().sum()>0:
    print('there are {} null values in {} column'.format(df[i].isnull().sum(),i))

#handlining null values
#we can handle the missing values by filling mean

df['ph'].describe()

df.isnull().sum()

df['ph_mean']=df['ph'].fillna(df['ph'].mean())

df['ph_mean'].isnull().sum()

fig = plt.figure()
ax = fig.add_subplot(111)
df['ph'].plot(kind='kde', ax=ax)
df.ph_mean.plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')
plt.show()

def impute_nan(df,variable):
  df[variable+"_random"]=df[variable]
  random_sample = df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)
  random_sample.index=df[df[variable].isnull()].index
  df.loc[df[variable].isnull(),variable+'_random']=random_sample

impute_nan(df,"ph")

fig = plt.figure()
ax = fig.add_subplot(111)
df['ph'].plot(kind='kde', ax=ax)
df.ph_random.plot(kind='kde', ax=ax, color='green')
df.ph_mean.plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')
plt.show()

impute_nan(df,"Sulfate")

fig = plt.figure()
ax = fig.add_subplot(111)
df['Sulfate'].plot(kind='kde', ax=ax)
df["Sulfate_random"].plot(kind='kde', ax=ax, color='green')
#data.ph_mean.plot(kind='kde', ax=ax, color='red')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')
plt.show()

impute_nan(df,"Trihalomethanes")

fig = plt.figure()
ax = fig.add_subplot(111)
df['Trihalomethanes'].plot(kind='kde', ax=ax)
df.Trihalomethanes_random.plot(kind='kde', ax=ax, color='green')
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines, labels, loc='best')
plt.show()

data = df.drop(['ph','Sulfate','Trihalomethanes','ph_mean'],axis=1)

data.isnull().sum()

#checking coorelation

plt.figure(figsize=(20, 17))
matrix = np.triu(data.corr())
sns.heatmap(data.corr(), annot=True,linewidth=.8, mask=matrix, cmap="rocket",cbar=False);

sns.pairplot(data, hue='Potability')

#data preprocesss

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

x=data.drop(['Potability'],axis=1)
y=data['Potability']

print(x.shape)
print(y.shape)

#we can use standard scaling to correfct the shape

scaler= StandardScaler()
x= scaler.fit_transform(x)

x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.85,random_state=42)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

#modelling

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(random_state=0).fit(x_train,y_train)
log.score(x_test,y_test)

from sklearn.metrics import confusion_matrix
# Make Predictions
pred1=log.predict(np.array(x_test))
plt.title("Confusion Matrix testing data")
sns.heatmap(confusion_matrix(y_test,pred1),annot=True,cbar=False)
plt.legend()
plt.show()